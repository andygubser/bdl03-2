{"paragraphs":[{"text":"%md\n# Dataflow","user":"bd01","dateUpdated":"2021-02-04T10:29:02+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Dataflow</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1612434542042_148678482","id":"20210127-071031_1944670894","dateCreated":"2021-02-04T10:29:02+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:16632"},{"text":"%md\n\n![dataflow](https://raw.githubusercontent.com/andygubser/bdl03-2/main/img/dataflow.png)\n","user":"bd01","dateUpdated":"2021-02-07T15:31:04+0000","config":{"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"results":{},"enabled":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1612434542044_-1212759018","id":"20210127-071046_672134150","dateCreated":"2021-02-04T10:29:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:16633","dateFinished":"2021-02-07T15:30:53+0000","dateStarted":"2021-02-07T15:30:53+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><img src=\"https://raw.githubusercontent.com/andygubser/bdl03-2/main/img/dataflow.png\" alt=\"dataflow\" /></p>\n</div>"}]}},{"text":"%md\nAt first I loaded via curl into a Spark DF and tried to perform the data wrangling steps with PySpark in Zeppelin (see Prototype 1). However, it was quite laborious and challenging and finally, I gave up. \n\n\nThus, I did the following procedure:\n* Downloaded the data to my local computer. \n* Manipulated the data using Pandas. \n* Uploaded the prepared data to the gateway. \n* Loaded the data into Spark. \n* Analysed the data with Spark.\n* Reported and visualized the results. ","user":"bd01","dateUpdated":"2021-02-07T16:44:23+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1612711806606_-1170707299","id":"20210207-153006_1586754196","dateCreated":"2021-02-07T15:30:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:17590","dateFinished":"2021-02-07T16:44:24+0000","dateStarted":"2021-02-07T16:44:23+0000","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>At first I loaded via curl into a Spark DF and tried to perform the data wrangling steps with PySpark in Zeppelin (see Prototype 1). However, it was quite laborious and challenging and finally, I gave up. </p>\n<p>Thus, I did the following procedure:<br/>* Downloaded the data to my local computer.<br/>* Manipulated the data using Pandas.<br/>* Uploaded the prepared data to the gateway.<br/>* Loaded the data into Spark.<br/>* Analysed the data with Spark.<br/>* Reported and visualized the results.</p>\n</div>"}]}},{"text":"%sh\n","user":"bd01","dateUpdated":"2021-02-07T16:36:46+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1612715806292_1286744012","id":"20210207-163646_1763276246","dateCreated":"2021-02-07T16:36:46+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:18587"}],"name":"MEP / wdgubser / 400 Dataflow","id":"2FWTQSHP4","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"shUser:bd01:":[],"sh:shared_process":[],"jdbc:shared_process":[],"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}